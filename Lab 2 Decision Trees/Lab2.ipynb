{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in CSV dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define data with pandas dataframe\n",
    "# mushroom dataframe\n",
    "# col_names = ['edibility','cap-shape','cap-surface','cap-color','bruises','odor','gill-attachment','gill-spacing','gill-size','gill-color','stalk-shape','stalk-root','stalk-surface-above-ring','stalk-surface-below-ring','stalk-color-above-ring','stalk-color-below-ring','veil-type','veil-color','ring-number','ring-type','spore-print-color','population','habitat']\n",
    "# # load dataset\n",
    "# df = pd.read_csv('agaricus-lepiota.data.csv', header=0, names=col_names)\n",
    "# df['class'] = df.edibility\n",
    "# df = df.drop(columns = 'edibility')\n",
    "\n",
    "#dropped due to missing\n",
    "#df = df.drop(columns = \"stalk-root\")\n",
    "\n",
    "\n",
    "#make sure last column is class\n",
    "\n",
    "\n",
    "#credit card dataframe\n",
    "col_names = ['A1','A2','A3','A4','A5','A6','A7','A8','A9','A10','A11','A12','A13','A14','A15','class']\n",
    "df = pd.read_csv('crx.data.csv', header = 0, names = col_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testSize = proportion or exact # of rows\n",
    "#didn't use k-folds or anything fancy, just random splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTestSplit(df, testSize = 0.2):\n",
    "    #default 80/20 split, seemed like a good amount\n",
    "    \n",
    "    if isinstance(testSize, float):\n",
    "        testSize = round(testSize * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    #convert indices to list to choose random test indices\n",
    "    testIndices = random.sample(population = indices, k = testSize)\n",
    "\n",
    "    #slice to get test\n",
    "    test = df.loc[testIndices]\n",
    "    #drop test from train\n",
    "    train = df.drop(testIndices)\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data\n",
    "train, test = trainTestSplit(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Check Purity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### using this instead of entropy == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkPurity(data):\n",
    "    classColumn = data[:, -1]\n",
    "    uniqueClasses = np.unique(classColumn)\n",
    "    \n",
    "    if len(uniqueClasses) == 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: Classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### returns class that appears most in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyData(data):\n",
    "    classColumn = data[:, -1]\n",
    "    uniqueClasses, countsUniqueClasses = np.unique(classColumn, returnCounts=True)\n",
    "    #unique values and times they appear\n",
    "\n",
    "    \n",
    "    index = countsUniqueClasses.argmax()\n",
    "    classification = uniqueClasses[index]\n",
    "    #chooses classification that appears most often\n",
    "    \n",
    "    return classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper: categorical vs continuous feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catOrCon(df):\n",
    "    featureTypes = []\n",
    "    numUniqueValuesThreshold = 15\n",
    "    \n",
    "    for column in df.columns:\n",
    "        uniqueValues = df[column].unique()\n",
    "        exampleValue = uniqueValues[0]\n",
    "        \n",
    "        if (isinstance(exampleValue, str)) or (len(uniqueValues) <= numUniqueValuesThreshold):\n",
    "            featureTypes.append(\"categorical\")\n",
    "        else:\n",
    "            featureTypes.append(\"continuous\")\n",
    "    \n",
    "    \n",
    "    return featureTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers: Splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### identifies splits for continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allPossibleSplits(data):\n",
    "    \n",
    "    potentialSplits = {}\n",
    "    _, numColumns = data.shape\n",
    "    for columnIndex in range(numColumns - 1):\n",
    "        values = data[:,columnIndex]\n",
    "        uniqueValues = np.unique(values)\n",
    "    \n",
    "        #for continuous\n",
    "        typeOfFeature = FEATURE_TYPES[splitColumn]\n",
    "        \n",
    "        if typeOfFeature == \"continuous\":\n",
    "            potentialSplits[columnIndex] = []\n",
    "            for index in range(len(uniqueValues)):\n",
    "                if index != 0:\n",
    "                    currentValue = uniqueValues[index]\n",
    "                    previousValue = uniqueValues[index-1]\n",
    "                    potentialSplit = (currentValue + previousValue) / 2\n",
    "\n",
    "                    potentialSplits[columnIndex].append(potentialSplit)\n",
    "        \n",
    "        #for categorical\n",
    "        elif len(uniqueValues) > 1:\n",
    "            potentialSplits[columnIndex] = uniqueValues \n",
    "    \n",
    "    return potentialSplits\n",
    "\n",
    "\n",
    "\n",
    "def splitData(data, splitColumn, splitValue):\n",
    "    splitColumnValues = data[:, splitColumn]\n",
    "    \n",
    "    typeOfFeature = FEATURE_TYPES[splitColumn]\n",
    "    if typeOfFeature == \"continuous\":\n",
    "        dataBelow = data[splitColumnValues <= splitValue]\n",
    "        dataAbove = data[splitColumnValues > splitValue]\n",
    "        \n",
    "    else:\n",
    "        dataBelow = data[splitColumnValues == splitValue]\n",
    "        dataAbove = data[splitColumnValues != splitValue]\n",
    "        \n",
    "    return dataBelow, dataAbove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers: Entropy/Info Gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateEntropy(data):\n",
    "    classColumn = data[:, -1]\n",
    "    _, counts = np.unique(classColumn, returnCounts = True)\n",
    "    probPos = counts / counts.sum()\n",
    "    entropy = sum( probPos * -np.log2(probPos) )\n",
    "    \n",
    "    return entropy\n",
    "\n",
    "\n",
    "def calculateInfoGain(dataBelow, dataAbove):\n",
    "    nDataPoints = len(dataBelow) + len(dataAbove)\n",
    "\n",
    "    pDataBelow = len(dataBelow) / nDataPoints\n",
    "    pDataAbove = len(dataAbove) / nDataPoints\n",
    "\n",
    "    infoGain = (pDataBelow * calculateEntropy(dataBelow)\n",
    "                      + pDataAbove * calculateEntropy(dataAbove))\n",
    "\n",
    "    \n",
    "    return infoGain\n",
    "\n",
    "\n",
    "def determineBestSplit(data, potentialSplits):\n",
    "    infoGain = 999\n",
    "    for columnIndex in potentialSplits:\n",
    "        for value in potentialSplits[columnIndex]:\n",
    "            dataBelow, dataAbove = splitData(data, splitColumn=columnIndex, splitValue = value)\n",
    "            currentInfoGain = calculateInfoGain(dataBelow, dataAbove)\n",
    "\n",
    "            if currentInfoGain <= infoGain:\n",
    "                infoGain = currentInfoGain\n",
    "                bestSplitColumn = columnIndex\n",
    "                bestSplitValue = value\n",
    "\n",
    "    return bestSplitColumn, bestSplitValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decisionTreeAlgorithm(df, counter = 0, minSamples = 2, maxDepth = 999):\n",
    "    \n",
    "    #needed to use global variables and store on first call\n",
    "    if counter == 0:\n",
    "        global COLUMN_HEADERS, FEATURE_TYPES\n",
    "        COLUMN_HEADERS = df.columns\n",
    "        FEATURE_TYPES = catOrCon(df)\n",
    "        \n",
    "        data = df.values\n",
    "    else:\n",
    "        data = df\n",
    "        \n",
    "    #base case\n",
    "    if (checkPurity(data)) or (len(data) < minSamples) or (counter == maxDepth):\n",
    "        classification = classifyData(data)\n",
    "        return classification\n",
    "    \n",
    "    #recursive\n",
    "    #determine the splits and split the data\n",
    "    #determine continuous or categorical and phrase question correctly\n",
    "    #assign positive/negative answers\n",
    "    else:\n",
    "        counter += 1\n",
    "        potentialSplits = allPossibleSplits(data)\n",
    "        splitColumn, splitValue = determineBestSplit(data, potentialSplits)\n",
    "        dataBelow, dataAbove = splitData(data, splitColumn, splitValue)\n",
    "        \n",
    "        featureName = COLUMN_HEADERS[splitColumn]\n",
    "        typeOfFeature = FEATURE_TYPES[splitColumn]\n",
    "        if typeOfFeature == \"continuous\":\n",
    "            question = \"{} <= {}\".format(featureName, splitValue)\n",
    "        else:\n",
    "            question = \"{} = {}\".format(featureName, splitValue)\n",
    "        subTree = {question: []}\n",
    "        \n",
    "        positiveAnswer = decisionTreeAlgorithm(dataBelow, counter, minSamples, maxDepth)\n",
    "        negativeAnswer = decisionTreeAlgorithm(dataAbove, counter, minSamples, maxDepth)\n",
    "        \n",
    "        if positiveAnswer == negativeAnswer:\n",
    "            subTree = positiveAnswer\n",
    "        else:\n",
    "            subTree[question].append(positiveAnswer)\n",
    "            subTree[question].append(negativeAnswer)\n",
    "\n",
    "        return subTree\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = decisionTreeAlgorithm(train)\n",
    "pprint(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification/Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifyExample(example, tree):\n",
    "    question = list(tree.keys())[0]\n",
    "    featureName, comparisonOperator, value = question.split()\n",
    "    if comparisonOperator == \"<=\":\n",
    "        if example[featureName] <= float(value):\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    else:\n",
    "        if str(example[featureName]) == value:\n",
    "            answer = tree[question][0]\n",
    "        else:\n",
    "            answer = tree[question][1]\n",
    "    if not isinstance(answer, dict):\n",
    "        return answer\n",
    "\n",
    "    else:\n",
    "        residualTree = answer\n",
    "        return classifyExample(example, residualTree)\n",
    "\n",
    "    \n",
    "    \n",
    "def calculateAccuracy(df, tree):\n",
    "    \n",
    "    df[\"classification\"] = df.apply(classifyExample, axis = 1, args= (tree,))\n",
    "    df[\"classification_correct\"] = df[\"classification\"] == df[\"class\"]\n",
    "    \n",
    "    accuracy = df[\"classification_correct\"].mean()\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "testAccuracy = calculateAccuracy(test, tree)\n",
    "testAccuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainAccuracy = calculateAccuracy(train, tree)\n",
    "trainAccuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
